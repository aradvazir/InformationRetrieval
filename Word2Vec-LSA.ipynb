{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import:\n",
    "\n",
    "In this cell, everything needed later is imported.\n",
    "\n",
    "1. pands as pd: for reading the input files.\n",
    "2. numpy as np.\n",
    "3. string, nltk: for tokenizing and preprocessing the data.\n",
    "4. gensim: to train the word2vec model.\n",
    "5. sklearn: for svm classifier, SVD decomposision and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize:\n",
    "\n",
    "In this cell, every text files, convert to a list whitout any puntuation and stop word.\n",
    "\n",
    "1. Punctuation are not important at all, so removing them helps the document retreival.\n",
    "2. Removing stop words is important beacuse they don't add much value to a text and make our search more sufficient and faster.\n",
    "3. Also we convert every character to it's lower case. And we do the same with query So that makes search and comparing more easy.\n",
    "4. And store each term stem instead of the term itself.\n",
    "\n",
    "Also we get terms of each document as a set and get union with other document terms. So we will have all unique terms in the document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(documents):\n",
    "    # Set the stop words for English\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    # This is the final list of tokenized text in lists\n",
    "    tokenized_list = []\n",
    "    terms = set([])\n",
    "    \n",
    "    # For each document, we use nltk regex_tokenize to token all the text file\n",
    "    for doc in documents:\n",
    "        tokenized_text =  nltk.regexp_tokenize(doc, r'\\d+,\\d+|\\w+')\n",
    "        \n",
    "        # Here we handle ',' in the numbers (beacuse nltk doesn't handle this)\n",
    "        for i in range(len(tokenized_text)):\n",
    "            if ',' in tokenized_text[i]:\n",
    "                w = ''\n",
    "                for c in tokenized_text[i]:\n",
    "                    if c != ',':\n",
    "                        w += c \n",
    "                    tokenized_text[i] = w\n",
    "                    \n",
    "        # And remove the stop words cause they don't add much to a text\n",
    "        text_without_stop_words = [word for word in tokenized_text if word not in stop_words]\n",
    "        \n",
    "        # Then remove any punctuation and store the stem of each term\n",
    "        text_without_punctuation = [porter.stem(word) for word in text_without_stop_words if word.isalnum()]\n",
    "        \n",
    "        tokenized_list.append(text_without_punctuation)\n",
    "        \n",
    "        # Getting unique terms till now\n",
    "        terms = terms.union(set(text_without_punctuation))\n",
    "    \n",
    "    return tokenized_list, list(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Index:\n",
    "\n",
    "Construct inverted index, a dictionary for document and collection frequency of each term using this function.\n",
    "\n",
    "for each term in each document:\n",
    "1. First check if it is already in the dictionary.\n",
    "2. If not, add it to the dictionary with the value of dictionary storing it's document and it's frequency in it.\n",
    "3. If is, Check if the document is new to the dictionary value of the term.\n",
    "4. If is add a document to it's documents.\n",
    "5. If not, increament the term frequency of that document.\n",
    "\n",
    "- The needed parts for constructing the TF-IDF matrix, is commented.\n",
    "\n",
    "Meanwhile, Update the collection frequecy (cf) and document frequency (df) for each term.\n",
    "1. df will be increamented if it is a new document.\n",
    "2. cf will be increased each time the term is seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(n_documents, tokenized_list):\n",
    "    # This is the dictionary for inverted index that we will construct\n",
    "    # inv_ind = dict()\n",
    "    # Dictionaries to store document and collection frequency of each term.\n",
    "    # df = dict()\n",
    "    cf = dict()\n",
    "    \n",
    "    for doc_id in range(n_documents):\n",
    "        for token_index in range(len(tokenized_list[doc_id])):\n",
    "            token = tokenized_list[doc_id][token_index]\n",
    "            # Checking if the term is already in the dictionary\n",
    "            # if token in inv_ind:\n",
    "            if token in cf:\n",
    "                # If the document has already been added to the term's documents\n",
    "                # if doc_id in list(inv_ind[token].keys()):\n",
    "                #     inv_ind[token][doc_id] += 1\n",
    "                #     cf[token] += 1\n",
    "                # If this is a new document\n",
    "                # else:\n",
    "                #     inv_ind[token][doc_id] = 1\n",
    "                #     df[token] += 1\n",
    "                    cf[token] += 1\n",
    "            # If this is a new term\n",
    "            else:\n",
    "                # inv_ind[token] = {doc_id:1}\n",
    "                # df[token] = 1\n",
    "                cf[token] = 1\n",
    "                \n",
    "    return cf #, df, inv_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct tf_idf:\n",
    "\n",
    "We will construct tf-idf matrix, by calculating the tf and idf:\n",
    "\n",
    "1. for each term, for each document that has the term inside:\n",
    "2. get the tf (stored in the inside dictionary with key the document).\n",
    "3. get the idf using the formula: $$idf = \\log(\\frac{N}{df})$$ where df is stored in df dictionary for the term.\n",
    "4. At last, divide the column vector (each document embeding) by it's norm to normalize the matrix.\n",
    "\n",
    "$\\bullet$ $tf\\_idf_{t,d} = tf_{t, d}\\cdot\\log(\\frac{N}{df_{t}})$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tf_idf_mat(inverted_index, df, terms, n_documents, n_terms):\n",
    "    # The tf_idf matrix\n",
    "    tf_idf = np.zeros((n_terms, n_documents), dtype='float')\n",
    "    \n",
    "    for i in range(n_terms):\n",
    "        for j in list(inverted_index[terms[i]].keys()):\n",
    "            tf = inverted_index[terms[i]][j]\n",
    "            idf = np.log2(n_documents/df[terms[i]])\n",
    "            tf_idf[i][j] = tf*idf\n",
    "            \n",
    "    for j in range(n_documents):\n",
    "        # Calculating the norm of column vector\n",
    "        s = 0\n",
    "        for i in range(n_terms):\n",
    "            s += tf_idf[i][j]**2\n",
    "        s = s**(1/2)\n",
    "        # Normalizing the column\n",
    "        for i in range(n_terms):\n",
    "            tf_idf[i][j] = tf_idf[i][j]/s\n",
    "            \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive bayes train function:\n",
    "\n",
    "In this function, the probabilities needed in the score calculating for naive bayes algorithm, will be calculated.\n",
    "$$ P(t|c) = \\frac{T_{t,c} + 1}{\\sum_{t'\\in V} T_{t',c} + 1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_bayes_trainer(cf, terms):\n",
    "    sum_of_all = 0\n",
    "    for term in terms:\n",
    "        if term in cf:\n",
    "            sum_of_all += cf[term] + 1\n",
    "        else:\n",
    "            sum_of_all += 1\n",
    "    \n",
    "    naive_bayes = dict({})\n",
    "    for term in list(cf.keys()):\n",
    "        naive_bayes[term] = (cf[term] + 1)/sum_of_all\n",
    "        \n",
    "    return naive_bayes, sum_of_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict:\n",
    "\n",
    "We score each document based on this (the prior probabilities for documents are considered to be equal):\n",
    "\n",
    "$$ P(d|c) = -\\sum_{t \\in d} \\log(P(t|c)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive_bayes(doc, stat_pos, stat_neg, p_pos, p_neg, den_pos, den_neg):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    for term in doc:\n",
    "        if term in stat_pos:\n",
    "            pos += np.log(stat_pos[term])\n",
    "        else:\n",
    "            pos -= np.log(den_pos)\n",
    "            \n",
    "        if term in stat_neg:\n",
    "            neg += np.log(stat_neg[term])\n",
    "        else:\n",
    "            neg -= np.log(den_neg)\n",
    "            \n",
    "    pos *= p_pos\n",
    "    neg *= p_neg\n",
    "    \n",
    "    if pos > neg:\n",
    "        return 1\n",
    "    elif neg > pos:\n",
    "        return -1\n",
    "    return np.random.choice([1, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we are doing in this cell:\n",
    "1. Collect data\n",
    "2. Tokenize each document\n",
    "3. Get the collection frequency for each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = pd.read_csv(\"./train_pos.csv\", index_col=0).iloc[:, 1].values\n",
    "train_neg = pd.read_csv(\"./train_neg.csv\", index_col=0).iloc[:, 1].values\n",
    "test_pos = pd.read_csv(\"./test_pos.csv\", index_col=0).iloc[:, 1].values\n",
    "test_neg = pd.read_csv(\"./test_neg.csv\", index_col=0).iloc[:, 1].values\n",
    "\n",
    "train_pos_doc, train_pos_terms = tokenize(train_pos)\n",
    "train_neg_doc, train_neg_terms = tokenize(train_neg)\n",
    "test_pos_doc, test_pos_terms = tokenize(test_pos)\n",
    "test_neg_doc, test_neg_terms = tokenize(test_neg)\n",
    "\n",
    "train_pos_cf = inverted_index(len(train_pos), train_pos_doc)\n",
    "train_neg_cf = inverted_index(len(train_neg), train_neg_doc)\n",
    "test_pos_cf = inverted_index(len(test_pos), test_pos_doc)\n",
    "test_neg_cf = inverted_index(len(test_neg), test_neg_doc)\n",
    "\n",
    "# tf_idf_train_pos = construct_tf_idf_mat(train_pos_inv_ind, trian_pos_df, train_pos_terms, len(train_pos), len(train_pos_terms))\n",
    "# tf_idf_train_neg = construct_tf_idf_mat(train_neg_inv_ind, trian_neg_df, train_neg_terms, len(train_neg), len(train_neg_terms))\n",
    "# tf_idf_test_pos = construct_tf_idf_mat(test_pos_inv_ind, test_pos_df, test_pos_terms, len(test_pos), len(test_pos_terms))\n",
    "# tf_idf_test_neg = construct_tf_idf_mat(test_neg_inv_ind, test_neg_df, test_neg_terms, len(test_neg), len(test_neg_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train the Naive Bayes classifier using train data.\n",
    "2. Test on the test data.\n",
    "3. Calculate the accuracy for positive class, negative class, and general train data.\n",
    "\n",
    "As we can see, the accuracy is 81.7% which is not so good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of positive test prediction: 0.7564\n",
      "accuracy of negative test prediction: 0.87416\n",
      "total accuracy of test prediction: 0.81528\n"
     ]
    }
   ],
   "source": [
    "terms = list(set(train_pos_terms).union(set(train_neg_terms)))\n",
    "\n",
    "naive_bayes_pos, den_pos = Naive_bayes_trainer(train_pos_cf, terms)\n",
    "naive_bayes_neg, den_neg = Naive_bayes_trainer(train_neg_cf, terms)\n",
    "\n",
    "n_train_pos = len(train_pos)\n",
    "n_train_neg = len(train_neg)\n",
    "n_train = n_train_pos + n_train_neg\n",
    "p_pos = n_train_pos/n_train\n",
    "p_neg = n_train_neg/n_train\n",
    "\n",
    "right_predict_pos = 0\n",
    "for doc in test_pos_doc:\n",
    "    p = predict_naive_bayes(doc, naive_bayes_pos, naive_bayes_neg, p_pos, p_neg, den_pos, den_neg)\n",
    "    if p == 1:\n",
    "        right_predict_pos += 1\n",
    "        \n",
    "right_predict_neg = 0\n",
    "for doc in test_neg_doc:\n",
    "    p = predict_naive_bayes(doc, naive_bayes_pos, naive_bayes_neg, p_pos, p_neg, den_pos, den_neg)\n",
    "    if p == -1:\n",
    "        right_predict_neg += 1\n",
    "        \n",
    "pos_per = right_predict_pos/len(test_pos)\n",
    "neg_per = right_predict_neg/len(test_neg)\n",
    "total_per = (right_predict_pos + right_predict_neg)/(len(test_pos) + len(test_neg))\n",
    "\n",
    "print(f\"accuracy of positive test prediction: {pos_per}\")\n",
    "print(f\"accuracy of negative test prediction: {neg_per}\")\n",
    "print(f\"total accuracy of test prediction: {total_per}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training:\n",
    "\n",
    "1. First we set the train data.\n",
    "2. Train the Word2Vec model with vector size 500 while the model window is 20.\n",
    "3. Then get the model vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(train_pos_doc) + list(train_neg_doc)\n",
    "model_w2v = Word2Vec(sentences=train_data, vector_size=200, window=20, min_count=1, workers=8)\n",
    "word_vectors = model_w2v.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In this cell, we construnct a $25000\\times 500$ matrix for train and test.\n",
    "2. This matrix is based on Word2Vec model Trained in the previous cell.\n",
    "3. The embedding for each document will be the average on it's terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings_train = []\n",
    "for doc in train_pos_doc + train_neg_doc:\n",
    "    embedding = np.array([0 for i in range(200)])\n",
    "    \n",
    "    for word in doc:\n",
    "        embedding = embedding + word_vectors[word]\n",
    "    \n",
    "    doc_embeddings_train.append([weight/len(doc) for weight in embedding])\n",
    "    \n",
    "\n",
    "doc_embeddings_test = []\n",
    "for doc in test_pos_doc + test_neg_doc:\n",
    "    embedding = np.array([0 for i in range(200)])\n",
    "    \n",
    "    for word in doc:\n",
    "        if word in word_vectors:\n",
    "            embedding = embedding + word_vectors[word]\n",
    "    \n",
    "    doc_embeddings_test.append([weight/len(doc) for weight in embedding])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set the SVM classifier\n",
    "2. Predict the test data\n",
    "3. calculate the accuracy on predicting test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on SVM for Word2Vec embeddings with length 100: 0.83668\n"
     ]
    }
   ],
   "source": [
    "train_labels = [1 for i in range(len(train_pos))] + [0 for i in range(len(train_neg))]\n",
    "test_labels = [1 for i in range(len(test_pos))] + [0 for i in range(len(test_neg))]\n",
    "\n",
    "# Set the SVM classifier\n",
    "svm_Word2Vec = SVC()\n",
    "svm_Word2Vec.fit(doc_embeddings_train, train_labels)\n",
    "\n",
    "# Predictions on the test set\n",
    "predictions = svm_Word2Vec.predict(doc_embeddings_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy on SVM for Word2Vec embeddings with length 100: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the precision, recall and F1-score metrics. They're all near 83.5%, same as the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.839477292893442\n",
      "Recall: 0.83256\n",
      "F1 Score: 0.8360043378720328\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = list(train_pos) + list(train_neg)\n",
    "data_text = list(test_pos) + list(test_neg)\n",
    "\n",
    "# Set the vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tf_idf_train = vectorizer.fit_transform(data_train)\n",
    "tf_idf_test = vectorizer.transform(data_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set the SVD using TruncatedSVD.\n",
    "2. Train The LSA based on constructed matricies in the previous cell.\n",
    "3. Transform The train and Test data based on the trained LSA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=200, random_state=123)\n",
    "train_lsa = lsa.fit_transform(tf_idf_train)\n",
    "test_lsa = lsa.transform(tf_idf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set the labels for train and test (the way we separated them).\n",
    "2. Set the SVM classifier.\n",
    "3. Train the SVM using Train data.\n",
    "4. Predict the test data.\n",
    "5. Calculate the accuracy.\n",
    "\n",
    "The accuracy using LSA combined with SVD on documents embedding is 84.2% which is a little bit better than Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVM on document embeddings gained from LSA: 0.86608\n"
     ]
    }
   ],
   "source": [
    "train_labels = [1 for i in range(len(train_pos))] + [0 for i in range(len(train_neg))]\n",
    "test_labels = [1 for i in range(len(test_pos))] + [0 for i in range(len(test_neg))]\n",
    "\n",
    "svm_LSA = SVC()\n",
    "svm_LSA.fit(train_lsa, train_labels)\n",
    "\n",
    "# Predictions on the test set\n",
    "predictions_lsa = svm_LSA.predict(test_lsa)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(test_labels, predictions_lsa)\n",
    "print(f\"Accuracy for SVM on document embeddings gained from LSA: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the precision, recall and F1-score metrics. They're all near 84%, same as the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8708866915221267\n",
      "Recall: 0.8596\n",
      "F1 Score: 0.8652065383686288\n"
     ]
    }
   ],
   "source": [
    "precision_LSA = precision_score(test_labels, predictions_lsa)\n",
    "recall_LSA = recall_score(test_labels, predictions_lsa)\n",
    "f1_LSA = f1_score(test_labels, predictions_lsa)\n",
    "\n",
    "print(\"Precision:\", precision_LSA)\n",
    "print(\"Recall:\", recall_LSA)\n",
    "print(\"F1 Score:\", f1_LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "Both of the approaches are somehow same at predicting and classifing the test data, based on the train data. But using SVM on LSA, is doing a better predictions on testset, the accuracy is 86.6%. While the accuracy using Naive Bayes and SVM on Word2Vec is about 84%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
