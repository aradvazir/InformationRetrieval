{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing:\n",
    "import os library for collecting the text files.\n",
    "\n",
    "import string and nltk for tokenizing the files.\n",
    "\n",
    "import product for calculating cartesian product between lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trie:\n",
    "This is the class of trie. we store every word in document and their permutation in this structure.\n",
    "\n",
    "It has an insert function, search function.\n",
    "\n",
    "Each node contains:\n",
    "1. children dictionary: It stores every character that comes after the node's char in all documents.\n",
    "2. is_end_of_word boolean: It shows that if this character is the end of some word in documents.\n",
    "3. doc dictionary: every documents and their indexes that the word has appeared in them, stores in this dictionary.\n",
    "4. char: it shows that which character is stored in this TrieNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self, char):\n",
    "        self.children = dict({})  # A dictionary to store child nodes\n",
    "        self.is_end_of_word = False  # Flag to indicate if a word ends at this node\n",
    "        self.doc = dict({}) # A dictionary to store the documents and the indexes that the term has appeard in that document\n",
    "        self.char = char # The node of a character\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode('')  # The root node of the Trie\n",
    "\n",
    "    def insert(self, word, doc_id, index):\n",
    "        node = self.root\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode(char) # If the char doesn't exist\n",
    "            node = node.children[char]\n",
    "        node.is_end_of_word = True\n",
    "        if doc_id not in node.doc: \n",
    "            node.doc[doc_id] = [index] # If the document doesn't exists\n",
    "        else:\n",
    "            node.doc[doc_id].append(index)\n",
    "\n",
    "    def search(self, word): # This function returns a last node and a bool which shows if it is a word\n",
    "        node = self.root\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                return False, None\n",
    "            node = node.children[char]\n",
    "        return node.is_end_of_word, node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting files:\n",
    "\n",
    "in this cell we use os library to get the text files in current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_files():\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # List all files in the current directory\n",
    "    file_list = os.listdir(current_directory)\n",
    "\n",
    "    # Filter the list to include only text files (if needed)\n",
    "    text_files = [file for file in file_list if file.endswith('.txt')]\n",
    "\n",
    "    # This is where we keep our documents\n",
    "    documents = []\n",
    "\n",
    "    # Loop through each text file and open/read them\n",
    "    for i in range(len(text_files)):\n",
    "        file_name = text_files[i]\n",
    "        file_path = os.path.join(current_directory, file_name)\n",
    "        \n",
    "        # Open and read the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            documents.append(file.read())\n",
    "    \n",
    "    return text_files, documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize:\n",
    "\n",
    "In this cell, every text files, convert to a list whitout any puntuation and stop word.\n",
    "\n",
    "Punctuation are not important at all, so removing them helps the algorithm.\n",
    "\n",
    "Removing stop words is important beacuse they don't add much value to a text and make our search more sufficient and faster.\n",
    "\n",
    "Also we convert every character to it's lower case. And we do the same with query So that makes search and comparing more easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(documents):\n",
    "    # Set the stop words for English\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # This is the final list of tokenized text in lists\n",
    "    tokenized_list = []\n",
    "    \n",
    "    # For each document, we use nltk regex_tokenize to token all the text file\n",
    "    for doc in documents:\n",
    "        tokenized_text =  nltk.regexp_tokenize(doc, r'\\d+,\\d+|\\w+')\n",
    "        \n",
    "        # Here we handle ',' in the numbers (beacuse nltk doesn't handle this)\n",
    "        for i in range(len(tokenized_text)):\n",
    "            if ',' in tokenized_text[i]:\n",
    "                w = ''\n",
    "                for c in tokenized_text[i]:\n",
    "                    if c != ',':\n",
    "                        w += c \n",
    "                    tokenized_text[i] = w\n",
    "        \n",
    "        # Then remove any punctuation\n",
    "        text_without_punctuation = [word.lower() for word in tokenized_text if word.isalnum()]\n",
    "        \n",
    "        # And remove the stop words cause they don't add much to a text\n",
    "        text_without_stop_words = [word for word in text_without_punctuation if word not in stop_words]\n",
    "        \n",
    "        tokenized_list.append(text_without_stop_words)\n",
    "    return tokenized_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Trie:\n",
    "\n",
    "We have two functions for constructing our Trie with every term in documents:\n",
    "1. construct_trie: Using this function, we add every term and it's permutation with '$' in the Trie.\n",
    "2. permute_add: This function add every permutation of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_add(s, trie, doc_id, index):\n",
    "    l = len(s)\n",
    "    for i in range(l):\n",
    "        trie.insert(s, doc_id, index) # Insert every permutation of s\n",
    "        s = s[1:] + s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_trie(tokenized_list):\n",
    "    trie = Trie()\n",
    "    \n",
    "    # Constructiong the Trie\n",
    "    for i in range(len(tokenized_list)):\n",
    "        for j in range(len(tokenized_list[i])):\n",
    "            s = tokenized_list[i][j] + \"$\"\n",
    "            permute_add(s, trie, i, j)\n",
    "    return trie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildcard:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to divide a word as we like:\n",
    "\n",
    "1. First we add a '$' to the word given\n",
    "2. Then we check if the space between two '*' is more than half of the length of the word. (we use this to get the most long prefix later)\n",
    "3. Then we rotate the word so a '*' will be at the end of the word. (We will have something like: word = *pref\"\\*\"...\"\\*\"* and we name the \"\\*\"...\"\\*\" part, rest. )\n",
    "4. After all we divide the word to two parts (pref and rest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    word = word + \"$\"\n",
    "    \n",
    "    # Ckeking for indexes of '*'s\n",
    "    i = -1\n",
    "    j = -1\n",
    "    first = 1\n",
    "    for k in range(len(word)):\n",
    "        if (word[k] == \"*\" and first):\n",
    "            i = k\n",
    "            first = 0\n",
    "        elif(word[k] == \"*\"):\n",
    "            j = k\n",
    "            break\n",
    "        \n",
    "    # Rotating the word as we like\n",
    "    if i > 0 and j > 0:\n",
    "        if (j - i > len(word)/2):\n",
    "            while(word[-1] != \"*\"):\n",
    "                word = word[1:] + word[0]\n",
    "        else:\n",
    "            while(word[-1] != \"*\"):\n",
    "                word = word[-1] + word[:-1]\n",
    "    \n",
    "    # Rotating for one star case\n",
    "    else:\n",
    "        while(word[-1] != \"*\"):\n",
    "            word = word[1:] + word[0]\n",
    "    \n",
    "    # finding the index of first '*' after rotation\n",
    "    i = 0\n",
    "    for k in range(len(word)):\n",
    "        i = k\n",
    "        if word[k] == \"*\":\n",
    "            break\n",
    "        \n",
    "    # Dividing the word\n",
    "    pref = word[:i]\n",
    "    rest = word[i:]\n",
    "    \n",
    "    return pref, rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function traverse a subtrie and returns every string below the node given:\n",
    "\n",
    "1. For each child in this node, we call the Trie_traverse with the below node and i+1. So it gives us the strings below this node. (As you can see, we get all the strings recursively.)\n",
    "2. Then we check if current node is an end for some word, if it is, we add a character of the node to the result list which we will return.\n",
    "\n",
    "Eplanation: The (i > 0) part is to add the character of the node to the returned suffixes, if it is node the given node in the first car. Cause we are trying to find every suffix for that node in the subtrie. So the concating the first node is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trie_traverse(node, i):\n",
    "    # This is the list that we keep every suffix below this TrieNode in the Trie.\n",
    "    result = []\n",
    "    \n",
    "    # Finding every string below and concating with the character in this node.\n",
    "    for child in list(node.children.keys()):\n",
    "        branch_words = Trie_traverse(node.children[child], i+1)\n",
    "        result += [node.char*(i > 0) + a for a in branch_words]\n",
    "    \n",
    "    # If this node, is an end of a word.\n",
    "    if node.is_end_of_word and i:\n",
    "        result += [node.char]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to convert the permutation to it's original (word$). And deleting the $ so we have the origial word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_original(term):\n",
    "    # Rotating untill we have the '$' at the end of word\n",
    "    while term[-1] != '$':\n",
    "        term = term[-1] + term[:-1]\n",
    "    \n",
    "    # Returning the original word (without '$')\n",
    "    return term[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function that handles wildcard queries:\n",
    "1. First of all we split the word as we mentioned earlier.\n",
    "2. Then we search the prefix in the Trie so we will have the node of the last character of the prefix.\n",
    "3. Then we use the Trie_traverse function to get every suffix in the subtrie of this node.\n",
    "4. While concating the prefix to returned suffixes, if there is two '*'s in the wildcard, we check for the middle part in the suffixes. If a suffix contains the middle part (between two stars), we add it to final results.\n",
    "5. And if there is nothing in the results (which means that the prefix was a word itself and had no children in the last node), we return the pref itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wildcard(word, trie):\n",
    "    # Dividing the word\n",
    "    pref, rest = clean_word(word)\n",
    "    \n",
    "    # Finding the prefix in trie\n",
    "    is_word, node = trie.search(pref)\n",
    "    \n",
    "    if node:\n",
    "        # Finding every suffix\n",
    "        every_words = Trie_traverse(node, 0)\n",
    "        \n",
    "        # Concating the prefix to the suffixes\n",
    "        if len(rest) > 2:\n",
    "            mid = rest[1:-1]\n",
    "            result = [pref + word for word in every_words if rest[1:-1] in word]\n",
    "        \n",
    "        else:\n",
    "            result = [pref + word for word in every_words]\n",
    "            \n",
    "        if not len(result):\n",
    "            result = [pref]\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    print(\"there is no such a word in documents dataset!\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell correction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort function is to sort the spell corrections such as low distances comes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort(l): # sort the spell correction list such as low distances comes first.\n",
    "    for i in range(len(l)):\n",
    "        for j in range(len(l)):\n",
    "            if l[i][0] <  l[j][0]:\n",
    "                t = l[i].copy()\n",
    "                l[i] = l[j].copy()\n",
    "                l[j] = t\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function calculates the distance between two strings using dynamic programming:\n",
    "\n",
    "1. first construct a matrix which the charaters of the first string are on the rows and the characters of the second string are on the columns.\n",
    "2. then fill the first column and row.\n",
    "3. At last, fill every other element in the matrix.\n",
    "\n",
    "(The pseudo code is available in lecture2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(s1, s2):\n",
    "    # The function which calculates the distance between two strings.\n",
    "    dp = [[0 for j in range(len(s2) + 1)] for i in range(len(s1) + 1)]\n",
    "    \n",
    "    for i in range(1, len(s1) + 1): \n",
    "        dp[i][0] = i\n",
    "\n",
    "    for j in range(1, len(s2) + 1): \n",
    "        dp[0][j] = j\n",
    "    \n",
    "    for i in range(1, len(s1) + 1):\n",
    "        for j in range(1, len(s2) + 1):\n",
    "            dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + (s1[i-1] != s2[j-1]))\n",
    "    \n",
    "    return dp[len(s1)][len(s2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function that handles Spell correction:\n",
    "\n",
    "1. First of all we find the distance between the word given and every other term in the documents.\n",
    "2. While calculating the distances, we also keep the minimum distance.\n",
    "3. For increased efficiency down the road, we only add the term with distance less than 5 (we have to sort this list later).\n",
    "4. After sorting the best_similar list based on distance, we return terms with least distance from the word given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction(word, terms):\n",
    "    best_similar = []\n",
    "    minimum = 100\n",
    "    for i in range(len(terms)):\n",
    "        distance = edit_distance(word, terms[i])\n",
    "        if distance < minimum:\n",
    "            minimum = distance\n",
    "        if (distance < 5): best_similar.append([distance, terms[i]])\n",
    "    best_similar = sort(best_similar)\n",
    "\n",
    "    return [best_similar[i][1] for i in range(len(best_similar)) if best_similar[i][0] == minimum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retreival handling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term case:\n",
    "In this case, we need to find the term in the trie and then print every document in it's doc dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_term(term, trie):\n",
    "    # Searching the term in the trie\n",
    "    term += \"$\"\n",
    "    node = trie.search(term)[1]\n",
    "    \n",
    "    # Printing every document in it's doc dictionary\n",
    "    for doc_id in list(node.doc.keys()):\n",
    "        print(text_files[doc_id][:-4])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT case:\n",
    "\n",
    "In this case we have to return every other document that the term is not appeard on them.\n",
    "1. First we get the documents for this term.\n",
    "2. Then Print every other document that are not in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_NOT(term, trie):\n",
    "    # Searching the term and getting it's documents\n",
    "    term += \"$\"\n",
    "    docs = list(trie.search(term)[1].doc.keys())\n",
    "    docs.sort()\n",
    "    \n",
    "    # Printing the other files than those in the docs\n",
    "    i = 0\n",
    "    k = 0\n",
    "    key = 1\n",
    "    while i < len(text_files) and k < len(docs):\n",
    "        if i < docs[k]:\n",
    "            print(text_files[i][:-4])\n",
    "            key = 0\n",
    "        \n",
    "        else:\n",
    "            k += 1\n",
    "        i += 1\n",
    "        \n",
    "    while i < len(text_files):\n",
    "        print(text_files[i][:-4])\n",
    "        key = 0\n",
    "        i += 1\n",
    "    if key:\n",
    "        print(\"no document found!\")\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And case:\n",
    "\n",
    "In this case, we obtain the intersection between the documents of two terms.\n",
    "1. Find the documents for each term.\n",
    "2. Check if any document in docs1, is in docs2.\n",
    "3. Print the ones that are in both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_AND(term1, term2, trie):\n",
    "    # Getting the documents for each term\n",
    "    term1 += \"$\"\n",
    "    term2 += \"$\"\n",
    "    docs1 = list(trie.search(term1)[1].doc.keys())\n",
    "    docs2 = list(trie.search(term2)[1].doc.keys())\n",
    "    docs1.sort()\n",
    "    docs2.sort()\n",
    "    \n",
    "    # Finding the intersection\n",
    "    key = 1\n",
    "    for i in docs1:\n",
    "        if i in docs2:\n",
    "            print(text_files[i][:-4])\n",
    "            key = 0\n",
    "    if key:\n",
    "        print(\"no document found!\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or case:\n",
    "\n",
    "In this fuction, we obtain the union between documents of two terms.\n",
    "1. First of all we get the documents for each term and sort them for getting union later.\n",
    "2. Then print every document which any of these two terms have appeard in by getting a union between lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_OR(term1, term2, trie):\n",
    "    # Finding documents\n",
    "    term1 += \"$\"\n",
    "    term2 += \"$\"\n",
    "    docs1 = list(trie.search(term1)[1].doc.keys())\n",
    "    docs2 = list(trie.search(term2)[1].doc.keys())\n",
    "    docs1.sort()\n",
    "    docs2.sort()\n",
    "    \n",
    "    # Getting union between lists\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(docs1) and j < len(docs2):\n",
    "        if docs1[i] < docs2[j]:\n",
    "            print(text_files[docs1[i]][:-4])\n",
    "            i += 1\n",
    "            \n",
    "        elif docs1[i] > docs2[j]:\n",
    "            print(text_files[docs2[j]][:-4])\n",
    "            j += 1\n",
    "            \n",
    "        else:\n",
    "            print(text_files[docs1[i]][:-4])\n",
    "            i += 1\n",
    "            j += 1\n",
    "            \n",
    "    while i < len(docs1):\n",
    "        print(text_files[docs1[i]][:-4])\n",
    "        i += 1\n",
    "            \n",
    "    while j < len(docs2):\n",
    "        print(text_files[docs2[j]][:-4])\n",
    "        j += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximity Case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to find out if there is an index that two terms have a distance less than \"distance\" from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_checking(list1, list2, distance):\n",
    "    # If there is a case that two terms in a document have a distance less than \"distance\"\n",
    "    for i in list1:\n",
    "        for j in list2:\n",
    "            if (abs(i-j) <= distance):\n",
    "                return 1\n",
    "            elif j > i:\n",
    "                break\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function for handling proximity:\n",
    "\n",
    "1. First of all we find documents of two terms and sort them in ascending way.\n",
    "2. Then we get the intersection between two lists.\n",
    "3. While getting an intersection, we check for the proximity condition in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_proximity(term1, term2, distance, trie):\n",
    "    # Finding documents for both terms\n",
    "    term1 += \"$\"\n",
    "    term2 += \"$\"\n",
    "    doc1 = trie.search(term1)[1].doc\n",
    "    doc2 = trie.search(term2)[1].doc\n",
    "    \n",
    "    docs1 = list(doc1.keys())\n",
    "    docs2 = list(doc2.keys())\n",
    "    docs1.sort()\n",
    "    docs2.sort()\n",
    "    \n",
    "    # Getting an intersection and checking for a proximity condition in the document\n",
    "    key = 1\n",
    "    for i in docs1:\n",
    "        if i in docs2:\n",
    "            if distance_checking(doc1[i], doc2[i], distance):\n",
    "                print(text_files[i][:-4])\n",
    "                key = 0\n",
    "    if key:\n",
    "        print(\"no document found!\")\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retreival:\n",
    "\n",
    "There are 3 cases in this function:\n",
    "\n",
    "1. If nothing is entered: In this case nothing happens\n",
    "2. If it is a Information retreival case: In this case we first use the wildcard or spell correction to get the correct terms and then as before, use the handling function for each correct case and return the related documents.\n",
    "3. If it is just the correcting query: In this case we have to find the most correct terms based on the terms from the documents. So we use wildcard or spell correction based on the term and find the correct cases and return all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_retreival(query, trie):\n",
    "    # Do nothing\n",
    "    if len(query) == 0:\n",
    "        return\n",
    "    \n",
    "    # Information retreival case\n",
    "    elif len(query) == 1 or \"AND\" in query or \"OR\" in query or \"NOT\" in query or \"NEAR\" in query[1]:\n",
    "        \n",
    "        # Finding the correct way of each term\n",
    "        result = []\n",
    "        for word in query:\n",
    "            if word not in [\"AND\", \"OR\", \"NOT\"] and \"NEAR\" not in word:\n",
    "                if \"*\" in word:\n",
    "                    result.append([make_original(term) for term in wildcard(word.lower(), trie)])\n",
    "                else:\n",
    "                    result.append(spell_correction(word.lower(), tokenized_list_stop_words))\n",
    "                    \n",
    "        # Handling single term\n",
    "        if len(query) == 1:\n",
    "            print(\"corrections for\", query[0], \":\")\n",
    "            print(result[0], \"\\n\")\n",
    "            \n",
    "            for word in result[0]:\n",
    "                print(\"results for\", word, \":\")\n",
    "                handling_term(word, trie)\n",
    "            print(\"done!\")\n",
    "            return\n",
    "    \n",
    "        # Handling NOT case\n",
    "        elif len(query) == 2:\n",
    "            print(\"corrections for\", query[1], \":\")\n",
    "            print(result[0], \"\\n\")\n",
    "            \n",
    "            if query[0] == 'NOT':\n",
    "                for word in result[0]:\n",
    "                    print(\"results for NOT\", word, \":\")\n",
    "                    handling_NOT(word, trie)\n",
    "                print(\"done!\")\n",
    "                return\n",
    "            \n",
    "        elif len(query) == 3:\n",
    "            print(\"corrections for\", query[0], \":\")\n",
    "            print(result[0])\n",
    "            print(\"corrections for\", query[2], \":\")\n",
    "            print(result[1], \"\\n\")\n",
    "            \n",
    "            # Handling AND case\n",
    "            if query[1] == 'AND':\n",
    "                for term1 in result[0]:\n",
    "                    for term2 in result[1]:\n",
    "                        print(\"results for\", term1, \" AND \", term2, \":\")\n",
    "                        handling_AND(term1, term2, trie)\n",
    "                print(\"done!\")\n",
    "                return\n",
    "            \n",
    "            # Handling OR case\n",
    "            elif query[1] == 'OR':\n",
    "                for term1 in result[0]:\n",
    "                    for term2 in result[1]:\n",
    "                        print(\"results for\", term1, \" OR \", term2, \":\")\n",
    "                        handling_OR(term1, term2, trie)\n",
    "                print(\"done!\")\n",
    "                return\n",
    "            \n",
    "            # Handling proximity case\n",
    "            elif 'NEAR/' in query[1]:\n",
    "                for term1 in result[0]:\n",
    "                    for term2 in result[1]:\n",
    "                        print(\"results for\", term1, query[1], term2, \":\")\n",
    "                        handling_proximity(term1, term2, int(query[1][5:]), trie)\n",
    "                print(\"done!\")\n",
    "                return\n",
    "    \n",
    "    # Correction case\n",
    "    else:\n",
    "        # Finding the correct way of each term\n",
    "        result = []\n",
    "        for word in query:\n",
    "            if \"*\" in word:\n",
    "                result.append([make_original(term) for term in wildcard(word.lower(), trie)])\n",
    "            else:\n",
    "                result.append(spell_correction(word.lower(), tokenized_list_stop_words))\n",
    "                \n",
    "        print(\"here are lists for each word:\")\n",
    "        for i in range(len(result)):\n",
    "            print(query[i], \":\")\n",
    "            print(result[i])\n",
    "        print(\"\\nand here are the results:\")\n",
    "        \n",
    "        # Finding the cartesian product of all correct ways\n",
    "        cartesian_product = list(product(*result))\n",
    "\n",
    "        # Print the result\n",
    "        for item in cartesian_product:\n",
    "            print(\" \".join(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to convert a matrix to a set to get all the unique terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique(matrix):\n",
    "    # Converting a matrix to a set\n",
    "    l = []\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[i])):\n",
    "            l.append(matrix[i][j])\n",
    "    return set(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main:\n",
    "\n",
    "1. First of all we get text_files and documents using get_text_files function based on available documents.\n",
    "2. Use the tokenize function to get the tokenized list.\n",
    "3. Get all unique terms form documents using make_set function.\n",
    "4. Construct the Trie.\n",
    "5. Get the query from user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting documents and file names\n",
    "text_files, documents = get_text_files()\n",
    "# tokenize every document\n",
    "tokenized_list = tokenize(documents)\n",
    "# Unique terms\n",
    "tokenized_list_stop_words =  list(find_unique(tokenized_list))\n",
    "# build the Trie\n",
    "trie = construct_trie(tokenized_list)\n",
    "\n",
    "# Getting the query and start searching\n",
    "information_retreival(list(input(\"Enter what you are looking for ('Enter' for stoping the search)\").split()), trie)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
