{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing:\n",
    ">import os library for collecting the text files.\n",
    "\n",
    ">import string and nltk for tokenizing the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedList and Node class:\n",
    "\n",
    "> in this cell we have two classes, Node and LinkedList which contains many Nodes.\n",
    "\n",
    "1. class Node contains a data, a list-which contains the indexes of a specific word in the document of this Node, a next pointer which pointes to the next node.\n",
    "\n",
    "2. class LinkedList contains two pointers to the front and last node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data, pointer):\n",
    "        self.data = data\n",
    "        self.list = [pointer]\n",
    "        self.next = None\n",
    "\n",
    "class LinkedList:\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.last = None\n",
    "        self.len = 0\n",
    "\n",
    "    def append(self, data, index): # appending a new node to the LinkedList\n",
    "        new_node = Node(data, index)\n",
    "        if not self.head:\n",
    "            self.head = new_node\n",
    "        else:\n",
    "            self.last.next = new_node\n",
    "        self.len += 1\n",
    "        self.last = new_node\n",
    "\n",
    "    def display(self): # this function displays the LinkedList\n",
    "        current = self.head\n",
    "        while current:\n",
    "            print(current.data, end=\" -> \")\n",
    "            current = current.next\n",
    "        print(\"None\")\n",
    "    \n",
    "    def is_empty(self):\n",
    "        if self.len == 0:\n",
    "            return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting files:\n",
    "\n",
    "in this cell we use os library to get the text files in current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_files():\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # List all files in the current directory\n",
    "    file_list = os.listdir(current_directory)\n",
    "\n",
    "    # Filter the list to include only text files (if needed)\n",
    "    text_files = [file for file in file_list if file.endswith('.txt')]\n",
    "\n",
    "    # This is where we keep our documents\n",
    "    documents = []\n",
    "\n",
    "    # Loop through each text file and open/read them\n",
    "    for i in range(len(text_files)):\n",
    "        file_name = text_files[i]\n",
    "        file_path = os.path.join(current_directory, file_name)\n",
    "        \n",
    "        # Open and read the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            documents.append(file.read())\n",
    "    \n",
    "    return text_files, documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize:\n",
    "\n",
    "In this cell, every text files, convert to a list whitout any puntuation and stop word.\n",
    "\n",
    "Punctuation are not important at all, so removing them helps the algorithm.\n",
    "\n",
    "Removing stop words is important beacuse they don't add much value to a text and make our search more sufficient and faster.\n",
    "\n",
    "Also we convert every character to it's lower case. And we do the same with query So that makes search and comparing more easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(documents):\n",
    "    # Set the stop words for English\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # This is the final list of tokenized text in lists\n",
    "    tokenized_list = []\n",
    "    \n",
    "    # For each document, we use nltk regex_tokenize to token all the text file\n",
    "    for doc in documents:\n",
    "        tokenized_text =  nltk.regexp_tokenize(doc, r'\\d+,\\d+|\\w+')\n",
    "        \n",
    "        # Here we handle ',' in the numbers (beacuse nltk doesn't handle this)\n",
    "        for i in range(len(tokenized_text)):\n",
    "            if ',' in tokenized_text[i]:\n",
    "                w = ''\n",
    "                for c in tokenized_text[i]:\n",
    "                    if c != ',':\n",
    "                        w += c \n",
    "                    tokenized_text[i] = w\n",
    "        \n",
    "        # Then remove any punctuation\n",
    "        text_without_punctuation = [word.lower() for word in tokenized_text if word.isalnum()]\n",
    "        \n",
    "        # And remove the stop words cause they don't add much to a text\n",
    "        text_without_stop_words = [word for word in text_without_punctuation if word not in stop_words]\n",
    "        \n",
    "        tokenized_list.append(text_without_stop_words)\n",
    "    return tokenized_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct inverted index:\n",
    "\n",
    "In this cell we construct the inverted index which is a dictionary with all unique term as it's keys and a LinkedList for it's values, which contains document_ids that the specific term appeard in it those documents.\n",
    "\n",
    "And also for each node (document_id for a term), there is a list which shows that the term appeard in which indexes in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(documents, tokenized_list):\n",
    "    # This is the dictionary for inverted index that we will construct\n",
    "    dictionary = dict()\n",
    "\n",
    "    for doc_id in range(len(documents)):\n",
    "        for token_index in range(len(tokenized_list[doc_id])):\n",
    "            if tokenized_list[doc_id][token_index] in dictionary: # Checking if the term is already in the dictionary\n",
    "                if dictionary[tokenized_list[doc_id][token_index]].last.data == doc_id: # Checking if the document_id is already in the term LinkedList\n",
    "                    dictionary[tokenized_list[doc_id][token_index]].last.list.append(token_index)\n",
    "                else: # If this is a new document\n",
    "                    dictionary[tokenized_list[doc_id][token_index]].append(doc_id, token_index)\n",
    "\n",
    "            else: # If this is a new term\n",
    "                dictionary[tokenized_list[doc_id][token_index]] = LinkedList()\n",
    "                dictionary[tokenized_list[doc_id][token_index]].append(doc_id, token_index)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term case:\n",
    "There is only one term in the query that needs to be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_term(term, dictionary):\n",
    "    point = dictionary[term].head\n",
    "    \n",
    "    while(point): # printing all the documents for the term\n",
    "        print(text_files[point.data][:-4])\n",
    "        point = point.next\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT case:\n",
    "\n",
    "This is the function which handles NOT. \n",
    "\n",
    "We just have to return every other document that are not in the specified term LinkedList.\n",
    "\n",
    "So, print the file names(whithout .txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_NOT(term, dictionary):\n",
    "    point = dictionary[term].head\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(documents): # Finding every other document_id to print the file names\n",
    "        if point and i == point.data: # Checking if the document_id ith is in LinkedList\n",
    "            point = point.next\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        print(text_files[i][:-4])\n",
    "        i += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AND case:\n",
    "\n",
    "This is the function which handles AND. In this function:\n",
    "1. We get doc_id list of each term at first.\n",
    "2. Find the intersection of two LinkedLists and print the file names. \n",
    "\n",
    "(we do this by moving the samller id to bigger ones so there might be the same id in other LinkedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_AND(term1, term2, dictionary):\n",
    "    if term1 in dictionary and term2 in dictionary:\n",
    "        \n",
    "        # Getting LikendLists\n",
    "        point1 = dictionary[term1].head\n",
    "        point2 = dictionary[term2].head\n",
    "        \n",
    "        while point1 and point2:\n",
    "            if point1.data == point2.data: # If the document_id is in both of LinkedLists\n",
    "                print(text_files[point1.data][:-4])\n",
    "                point1 = point1.next\n",
    "                point2 = point2.next\n",
    "                \n",
    "            elif point1.data > point2.data:\n",
    "                point2 = point2.next\n",
    "            \n",
    "            else:\n",
    "                point1 = point1.next\n",
    "    else: # If there is no key in the dictionary for at least one of the searched terms\n",
    "        print(\"there is no result found\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR case:\n",
    "\n",
    "This is the function which handles OR. In this function:\n",
    "\n",
    "1. We get doc_id list of each term at first.\n",
    "2. Find the union between two Lists.\n",
    "\n",
    "(For doing this, we have to print every document that have appeard in at least one of these Lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_OR(term1, term2, dictionary):\n",
    "    if term1 in dictionary and term2 in dictionary:\n",
    "        \n",
    "        # Getting LikendLists\n",
    "        point1 = dictionary[term1].head\n",
    "        point2 = dictionary[term2].head\n",
    "        \n",
    "        while point1 and point2:\n",
    "            if point1.data == point2.data: # If the id is in both of Lists, only one of them needs to be printed\n",
    "                print(text_files[point1.data][:-4])\n",
    "                point1 = point1.next\n",
    "                point2 = point2.next\n",
    "            \n",
    "            # If one of them has a lower id than the other, the lower one needs to be printed\n",
    "            elif point1.data > point2.data:\n",
    "                print(text_files[point2.data][:-4])\n",
    "                point2 = point2.next\n",
    "            \n",
    "            else:\n",
    "                print(text_files[point1.data][:-4])\n",
    "                point1 = point1.next\n",
    "             \n",
    "        # This is the case that one of the LinkedList has ended and there is no need to compare, only printing   \n",
    "        while point1:\n",
    "            print(text_files[point1.data][:-4])\n",
    "            point1 = point1.next\n",
    "                \n",
    "        while point2:\n",
    "            print(text_files[point2.data][:-4])\n",
    "            point2 = point2.next\n",
    "    else: # If there is no key in the dictionary for at least one of the searched terms\n",
    "        print(\"there is no result found\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximity case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function checks if there is any case that two specified terms, appeard in at most \"distance\" far from each other in a specific document.\n",
    "\n",
    "It is a side function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_checking(list1, list2, distance):\n",
    "    for i in list1:\n",
    "        for j in list2:\n",
    "            if (abs(i-j) <= distance):\n",
    "                return 1\n",
    "            elif j > i:\n",
    "                break\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function to handle proximity case. in this function:\n",
    "\n",
    "1. We get doc_id list for each term at first\n",
    "2. Then we get the intersection between two Lists. Exactly like what we did in AND case.\n",
    "3. Then for each document in intersection, we check (using distance_checking function from above) if there is any indexes that two terms have at most a specified distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_proximity(term1, term2, distance, dictionary):\n",
    "    if term1 in dictionary and term2 in dictionary:\n",
    "        \n",
    "        # Getting LikendLists\n",
    "        point1 = dictionary[term1].head\n",
    "        point2 = dictionary[term2].head\n",
    "\n",
    "        while point1 and point2:\n",
    "            if point1.data == point2.data:\n",
    "                if distance_checking(point1.list , point2.list, distance):\n",
    "                    print(text_files[point1.data][:-4])\n",
    "                point1 = point1.next\n",
    "                point2 = point2.next\n",
    "                \n",
    "            elif point1.data > point2.data:\n",
    "                point2 = point2.next\n",
    "            \n",
    "            else:\n",
    "                point1 = point1.next\n",
    "    else: # If there is no key in the dictionary for at least one of the searched terms\n",
    "        print(\"there is no result found\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main:\n",
    "\n",
    "This is where everything handles.\n",
    "\n",
    "There are three cases:\n",
    "\n",
    "1. if the query has only 1 term: In this case it is only a term that the inverted index must be returend.\n",
    "2. if the query has only 2 terms: In this case it should be NOT case\n",
    "3. if the query has only 3 terms: In this case it is either AND case, OR case or Proximity case. We check with the middle term of the query for this.\n",
    "4. if the query is empty: End of search.\n",
    "\n",
    "Also every terms will be passed to it's handling function in lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func(query, dictionary):\n",
    "    while True:\n",
    "        if len(query) == 1:\n",
    "            handling_term(query[0].lower(), dictionary)\n",
    "            print(\"done!\")\n",
    "            return\n",
    "            \n",
    "        elif len(query) == 2:\n",
    "            if query[0] == 'NOT':\n",
    "                handling_NOT(query[1].lower(), dictionary)\n",
    "                print(\"done!\")\n",
    "                return\n",
    "            \n",
    "        elif len(query) == 3:\n",
    "            if query[1] == 'AND':\n",
    "                handling_AND(query[0].lower(), query[2].lower(), dictionary)\n",
    "                print(\"done!\")\n",
    "                return\n",
    "            \n",
    "            elif query[1] == 'OR':\n",
    "                handling_OR(query[0].lower(), query[2].lower(), dictionary)\n",
    "                print(\"done!\")\n",
    "                return\n",
    "            \n",
    "            elif 'NEAR/' in query[1]:\n",
    "                handling_proximity(query[0].lower(), query[2].lower(), int(query[1][5:]), dictionary)\n",
    "                print(\"done!\")\n",
    "                return\n",
    "\n",
    "        elif len(query) == 0:\n",
    "            return 0\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First of all, we get the files and make text_files and documents lists.\n",
    "2. Then we token every document and store them in tokenized_list.\n",
    "3. At last, build the inverted index dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Festival of Books\n",
      "A Murder-Suicide\n",
      "Gasoline Prices Hit Record High\n",
      "Happy and Unhappy Renters\n",
      "Rentals at the Oceanside Community\n",
      "Trees Are a Threat\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "text_files, documents = get_text_files()\n",
    "tokenized_list = tokenize(documents)\n",
    "dictionary = inverted_index(documents, tokenized_list)\n",
    "\n",
    "main_func(list(input(\"Enter what you are looking for ('Enter' for stoping the search)\").split()), dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "8\n",
      "9\n",
      "13\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "p = dictionary[\"people\"].head\n",
    "while p:\n",
    "    print(p.data+1)\n",
    "    p = p.next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
